---
title: "Glassdoor class model"
author: "fpd4fv"
date: '2023-02-21'
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
sessionInfo()

```

```{r}
#bypassing this because we bring in the data already cleaned and split into train, tune, and test.

fraud_total <- read.csv('fake_job_postings.csv')
summary(fraud_total)

#reducing df to just description and fraudulent indicator
fraud_data <- fraud_total[, c("description", "fraudulent")]

summary(fraud_data)

```


```{r}
#bypassing this by conducting caret operations on local machine. caret library doesn't work on this virtual env
library(caret)

set.seed(1205)
fraud_index <- caret::createDataPartition(fraud_data$fraudulent, times=1, p=.7, groups = 1, list = FALSE)
fraud_train <- fraud_data[fraud_index,]

test_tune <- fraud_data[-fraud_index,]
ttind <- caret::createDataPartition(test_tune$fraudulent, times=1, p=.5, groups = 1, list = FALSE)
fraud_tune <- test_tune[ttind,]
fraud_test <- test_tune[-ttind,]
dim(fraud_train)
dim(fraud_tune)
dim(fraud_test)

train <- fraud_train
tune <- fraud_tune
test <- fraud_test


str(fraud_train)

```

```{r}
#no longer using this chunk because UVA's rivanna computer has enough RAM to train the model on the full training set

#reducing size of train to 75% in order to circumvent RAM capacity problem
fraud_train_index <- caret::createDataPartition(train$fraudulent, times=1, p=.40, groups = 1, list = FALSE)
fraud_train_40 <- fraud_train[fraud_train_index,]
train <- fraud_train_40

str(train)
str(tune)



```

```{r}
#bringing in train, tune, and test sets from local machine
train <- read.csv('job_listing_fraud_train.csv')
tune <- read.csv('job_listing_fraud_tune.csv')
test <- read.csv('job_listing_fraud_test.csv')

```


```{r}
#cleaning corpus

library(tm)

#Creating Corpus
suppressWarnings(Data_test_corpus <- Corpus(VectorSource(test$description)))
suppressWarnings(Data_train_corpus <- Corpus(VectorSource(train$description)))
suppressWarnings(Data_tune_corpus <- Corpus(VectorSource(tune$description)))

#Corpus Cleaning
suppressWarnings(Data_test_corpus_clean <- tm_map(Data_test_corpus, tolower))
suppressWarnings(Data_train_corpus_clean <- tm_map(Data_train_corpus, tolower))
suppressWarnings(Data_tune_corpus_clean <- tm_map(Data_tune_corpus, tolower))

suppressWarnings(Data_test_corpus_clean <- tm_map(Data_test_corpus_clean, removeNumbers))
suppressWarnings(Data_train_corpus_clean <- tm_map(Data_train_corpus_clean, removeNumbers))
suppressWarnings(Data_tune_corpus_clean <- tm_map(Data_tune_corpus_clean, removeNumbers))

suppressWarnings(Data_test_corpus_clean <- tm_map(Data_test_corpus_clean, removeWords, stopwords()))
suppressWarnings(Data_train_corpus_clean <- tm_map(Data_train_corpus_clean, removeWords, stopwords()))
suppressWarnings(Data_tune_corpus_clean <- tm_map(Data_tune_corpus_clean, removeWords, stopwords()))

suppressWarnings(Data_test_corpus_clean <- tm_map(Data_test_corpus_clean, removePunctuation))
suppressWarnings(Data_train_corpus_clean <- tm_map(Data_train_corpus_clean, removePunctuation))
suppressWarnings(Data_tune_corpus_clean <- tm_map(Data_tune_corpus_clean, removePunctuation))

suppressWarnings(Data_test_corpus_clean <- tm_map(Data_test_corpus_clean, stripWhitespace))
suppressWarnings(Data_train_corpus_clean <- tm_map(Data_train_corpus_clean, stripWhitespace))
suppressWarnings(Data_tune_corpus_clean <- tm_map(Data_tune_corpus_clean, stripWhitespace))

suppressWarnings(inspect(Data_train_corpus_clean[1]))

head(train)

```


```{r}


#Sparse Matrix
test_dtm <- DocumentTermMatrix(Data_test_corpus_clean)
train_dtm <- DocumentTermMatrix(Data_train_corpus_clean)

tune_dtm <- DocumentTermMatrix(Data_tune_corpus_clean)

class(train_dtm)
class(tune_dtm)
class(test_dtm)
#At this point, all train_dtm tune_dtm and test_dtm are "DocumentTermMatrix" and "simple_triplet_matrix"

inspect(train_dtm)

str(train_dtm)


```


```{r}


##### Preparing Training, tuning, and Testing Datasets #####
### Creating Indicator features for frequent words ###
FreqWords <- findFreqTerms(tune_dtm, 5)

#Saving List using Dictionary() Function
Dictionary <- function(x) {
        if( is.character(x) ) {
                return (x)
        }
        stop('x is not a character vector')
}

data_dict <- Dictionary(findFreqTerms(tune_dtm, 5))

#Appending Document Term Matrix to Train and Test Dataset 
data_train <- DocumentTermMatrix(Data_train_corpus_clean, list(data_dict))
data_test <- DocumentTermMatrix(Data_test_corpus_clean, list(data_dict))


data_tune <- DocumentTermMatrix(Data_tune_corpus_clean, list(data_dict))


#Converting the frequency of word to count
convert_counts <- function(x) {
        x <- ifelse(x > 0, 1, 0)
        x <- factor(x, levels = c(0, 1), labels = c("No", "Yes")) 
        return(x)
}

#Appending count function to Train and Test Dataset
data_train <- apply(data_train, MARGIN = 2, convert_counts)
data_test <- apply(data_test, MARGIN = 2, convert_counts)

data_tune <- apply(data_tune, MARGIN = 2, convert_counts)

#https://www.rc.virginia.edu/userinfo/rivanna/login/





```


```{r}
#Naive Bayes Classification
library(e1071)
data_classifier <- naiveBayes(data_train, train$fraudulent)

data_classifier_a <- naiveBayes(data_tune, tune$fraudulent)

class(data_tune)

class(data_train)
class(data_test)

#problems arise because data_tune and data_test are "matrix" "array" which makes them work for training but data_train is "DocumentTermMatrix" "simple_triplet_matrix" and data_train is producing error 
'''
> data_classifier <- naiveBayes(data_train, train$fraudulent)
Error in as.data.frame.default(x) : 
  cannot coerce class ‘c("DocumentTermMatrix", "simple_triplet_matrix")’ to a data.frame
'''


```


```{r}
library(gmodels)
data_test_pred <- predict(data_classifier, data_test)

CrossTable(data_test_pred, test$fraudulent,
           prop.chisq = FALSE, prop.t = FALSE,
           dnn = c('predicted', 'actual'))

```
